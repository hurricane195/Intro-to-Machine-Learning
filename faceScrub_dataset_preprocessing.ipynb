{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hurricane195/Intro-to-Machine-Learning/blob/Final-Project/faceScrub_dataset_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9F7xdq37zH6"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import random\n",
        "import shutil\n",
        "import os\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from random import randrange\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, ConcatDataset\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_drive  = '/content/drive'\n",
        "drive.mount(path_drive)\n",
        "\n",
        "path_dataset = path_drive + '/MyDrive/faceScrub' # path to the original faceScrub dataset\n",
        "path_actors_faces = path_dataset + '/faces/'\n",
        "path_faces_file = path_dataset + '/faces.txt'\n",
        "path_output_dataset = './data' # change this to whatever directory you wish to store your output dataset\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnsCVfL69jME",
        "outputId": "06d1b3d3-ed17-4754-e6f8-cd4e18fbeec8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_directory(dataset, dataset_dir, resolution, verbose=1):\n",
        "    if verbose:\n",
        "        print('Creating {} directory...'.format(dataset_dir))\n",
        "    Path(dataset_dir).mkdir(parents=True)\n",
        "    for face in dataset:\n",
        "        actor_name = face['name']\n",
        "        try:\n",
        "           (Path(dataset_dir) / actor_name).mkdir()\n",
        "        except:\n",
        "          pass\n",
        "        img1 = Image.open(face['face'])\n",
        "        if verbose==2:\n",
        "            print('Resizing {} '.format( str(Path(face['face']).name)) )\n",
        "        if Path(face['face']).suffix != '.jpg' or Path(face['face']).suffix != '.jpeg':\n",
        "            img1 = img1.convert('RGB')\n",
        "        img1 = img1.resize(resolution)\n",
        "        save_path = str(Path(dataset_dir) / actor_name / Path(face['face']).stem ) +  '.png'\n",
        "        img1.save( save_path, 'PNG')\n",
        "        face[actor_name] = save_path\n",
        "\n",
        "\n",
        "\n",
        "def create_info_file(dataset, dataset_dir, file_name, verbose=1):\n",
        "    if verbose:\n",
        "        print('Creating {} file...'.format(file_name))\n",
        "    with open( str(Path(dataset_dir) / file_name), 'w', newline='') as file:\n",
        "        fieldnames = dataset[0].keys()\n",
        "        writer = csv.DictWriter(file, fieldnames=fieldnames, delimiter='\\t')\n",
        "        writer.writeheader()\n",
        "        for face in dataset:\n",
        "            writer.writerow(face)\n",
        "\n",
        "\n",
        "\n",
        "def create_datasets(train_size, val_size, test_size, resolution, verbose=1):\n",
        "    # Read download directory\n",
        "    if verbose:\n",
        "        print('\\nReading actors folder...')\n",
        "\n",
        "    faces = [] # this list contains all the download information\n",
        "\n",
        "    # check if faces.txt file exists\n",
        "    if Path(path_faces_file).is_file():\n",
        "        with open(path_faces_file, newline='') as faces_file:\n",
        "            faces_reader = csv.DictReader(faces_file, delimiter='\\t')\n",
        "            for face in faces_reader:\n",
        "                faces.append(face)\n",
        "    else:\n",
        "        # it takes some minutes to scan the whole directory!\n",
        "        for actor_entry in tqdm(Path(path_actors_faces).iterdir(), desc =\"Reading faces\"):\n",
        "            if actor_entry.is_dir(): # read only directories\n",
        "                for face_entry in actor_entry.iterdir():\n",
        "                    faces.append( {'name':actor_entry.name, 'face':str(face_entry)} ) # add info to faces list\n",
        "\n",
        "        create_info_file(faces, str(Path(path_faces_file).parent), Path(path_faces_file).name) # create faces.txt file\n",
        "\n",
        "    # Shuffle list\n",
        "    random.shuffle(faces)\n",
        "\n",
        "    # clear data dir\n",
        "    if Path('data').is_dir():\n",
        "        shutil.rmtree('data')\n",
        "\n",
        "    if verbose:\n",
        "        print('\\nCreating test set...')\n",
        "    # Create test set list\n",
        "    test_set = []\n",
        "    actors = [] # list to keep track which actors have been used\n",
        "    for idx, face in enumerate(faces):\n",
        "        # Examine image and dicard if not RGB, e.g, type L (b/w)\n",
        "        img = Image.open(face['face'])\n",
        "        if img.mode == 'RGB':\n",
        "            test_set.append(face)\n",
        "            actors.append(face['name'])\n",
        "        if len(test_set) == test_size:\n",
        "            break\n",
        "    actors = list(set(actors)) # delete duplicates\n",
        "    create_directory(test_set, path_output_dataset + '/test/', resolution, verbose=verbose)\n",
        "\n",
        "    if verbose:\n",
        "        print('\\nCreating validation set...')\n",
        "    # Create validation set list, make sure that no actor from test set is here\n",
        "    validation_set = []\n",
        "    for idx, face in enumerate(faces[idx+1:], start=idx+1): # continue reading from idx\n",
        "        img = Image.open(face['face'])\n",
        "        if face['name'] not in actors and img.mode == 'RGB':\n",
        "            validation_set.append(face)\n",
        "        if len(validation_set) == val_size:\n",
        "            break\n",
        "    create_directory(validation_set, path_output_dataset + '/valid/', resolution, verbose=verbose)\n",
        "\n",
        "    if verbose:\n",
        "        print('\\nCreating training set...')\n",
        "    # Create training set list, make sure that no actor from test set is here\n",
        "    training_set = []\n",
        "    for face in faces[idx+1:]:\n",
        "        img = Image.open(face['face'])\n",
        "        if face['name'] not in actors and img.mode == 'RGB':\n",
        "            training_set.append(face)\n",
        "        if len(training_set) == train_size:\n",
        "            break\n",
        "    create_directory(training_set, path_output_dataset + '/train/', resolution, verbose=verbose)\n",
        "\n",
        "    if verbose:\n",
        "        print('\\nDatasets created sucessfully!\\n')\n",
        "\n",
        "    return training_set, validation_set, test_set\n"
      ],
      "metadata": {
        "id": "zEj7zbdm9WYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataset\n",
        "tain_set, valid_set, test_set = create_datasets(\n",
        "    train_size = 1200,\n",
        "    val_size = 300,\n",
        "    test_size = 300,\n",
        "    resolution = [128, 128])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c_2TNaI-rtr",
        "outputId": "2b393d87-1fa8-4d0f-a120-0558b1c01990"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Reading actors folder...\n",
            "\n",
            "Creating test set...\n",
            "Creating ./data/test/ directory...\n",
            "\n",
            "Creating validation set...\n",
            "Creating ./data/valid/ directory...\n",
            "\n",
            "Creating training set...\n",
            "Creating ./data/train/ directory...\n",
            "\n",
            "Datasets created sucessfully!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # Load data from folders\n",
        "train_dir = path_output_dataset + '/train'\n",
        "valid_dir = path_output_dataset + '/valid'\n",
        "\n",
        "train_dataset = datasets.ImageFolder(\n",
        "    train_dir\n",
        ")\n",
        "valid_dataset = datasets.ImageFolder(\n",
        "    valid_dir\n",
        ")\n",
        "\n",
        "    # Create data loaders\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=128, shuffle=True,\n",
        "    num_workers=0)\n",
        "valid_loader = torch.utils.data.DataLoader(\n",
        "    valid_dataset, batch_size=128, shuffle=False,\n",
        "    num_workers=0)"
      ],
      "metadata": {
        "id": "4W9-OQtzGETd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cTadrSyX86w",
        "outputId": "b5786e00-f198-4d5e-8d59-a8e0428d2d00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset ImageFolder\n",
              "    Number of datapoints: 1200\n",
              "    Root location: ./data/train"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MpZ7EkiX-vj",
        "outputId": "f84c91d7-bf84-4667-8d6f-ba4964c40fa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset ImageFolder\n",
              "    Number of datapoints: 300\n",
              "    Root location: ./data/valid"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    }
  ]
}