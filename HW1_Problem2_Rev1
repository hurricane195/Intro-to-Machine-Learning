import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Use the URL for the raw CSV data -- more datasets can be find here: https://github.com/satishgunjal/datasets
url = 'https://raw.githubusercontent.com/hurricane195/wickedhurricane/main/D3.csv'

df = pd.read_csv(url)

# Display the first 5 rows of the DataFrame
print(df.head())

# Separate features and labels
X1 = df.values[:,0]  # get X1 input values from the first column -- X is a list here
X2 = df.values[:,1]  # get X2 input values from the second column -- X is a list here
X3 = df.values[:,2]  # get X3 input values from the third column -- X is a list here
y = df.values[:,3]  # get output values from fourth column -- Y is the list here
m = len(y)  # Number of training examples
n1= len(X1)  # Number of training examples
n2= len(X2)  # Number of training examples
n3= len(X3)  # Number of training examples


# Display first 5 records and the total number of training examples
print('X1 = ', X1[: 5])
print('X2 = ', X2[: 5])
print('X3 = ', X3[: 5])
print('y = ', y[: 5])
print('m = ', m)
print('n1 = ', n1)
print('n2 = ', n2)
print('n3 = ', n3)

print('X1 = ', X1[: 100])
print('X2 = ', X2[: 100])
print('X3 = ', X3[: 100])
print('y = ', y[: 100])


from IPython.display import display
display(df)

X1 = df.values[:,0]  # get X1 input values from the first column -- X1 is a list here which is a 1 dimentional array
X2 = df.values[:,1]  # get X1 input values from the second column -- X2 is a list here which is a 1 dimentional array
X3 = df.values[:,2]  # get X1 input values from the second column -- X3 is a list here which is a 1 dimentional array
y = df.values[:,3]  # get output values from the second column --  Y is a list here which is a 2 dimentional array

X1= X1.reshape(m, 1)
X1[:10]

X2= X2.reshape(m, 1)
X2[:10]

X3= X3.reshape(m, 1)
X3[:10]

# Scatter plot
plt.scatter(X1, y, color='red', marker='+')
plt.scatter(X2, y, color='blue', marker='x')
plt.scatter(X3, y, color='purple', marker='*')

# Grid, labels, and title
plt.grid(True)
plt.rcParams["figure.figsize"] = (6, 6)
plt.xlabel('X1')
plt.xlabel('X2')
plt.xlabel('X3')

plt.ylabel('Y')
plt.title('Scatter plot of training data')

# Show the plot
plt.show()

#We walk through the initial steps of building a linear regression model from scratch using NumPy. Let's break down what you're doing:
#X_0 = np.ones((m, 1)): We're creating a column vector of ones. This will be used as the "bias" term for the linear regression model.
#X_1 = X.reshape(m, 1): You're reshaping features (X) to make it a 2D array suitable for matrix operations.
#X = np.hstack((X_0, X_1)): We're horizontally stacking X_0 and X_1 to create final feature matrix X.

X_0 = np.ones((m, 1))
X_0[:5]

X_1 = X1.reshape(m, 1)
X_1[:10]

X_2 = X2.reshape(m, 1)
X_2[:10]

X_3 = X3.reshape(m, 1)
X_3[:10]


# Lets use hstack() function from numpy to stack X_0 and X_1 horizontally (i.e. column
# This will be our final X matrix (feature matrix)
X = np.hstack((X_0, X_1, X_2, X_3))
X[:5]

theta = np.zeros(4)
theta

def compute_cost(X, y, theta):
    """
    Compute cost for linear regression.

    Parameters:
    X : 2D array where each row represents the training example and each column represent the feature
        m = number of training examples
        n = number of features (including X_0 column of ones)
    y : 1D array of labels/target values for each training example. dimension(m)
    theta : 1D array of fitting parameters or weights. Dimension (n)

    Returns:
    J : Scalar value, the cost
    """
    predictions = X.dot(theta)
    errors = np.subtract(predictions, y)
    sqrErrors = np.square(errors)
    J = 1 / (2 * m) * np.sum(sqrErrors)
    return J

# Lets compute the cost for theta values
cost = compute_cost(X, y, theta)
print('The cost for given values of theta_0 and theta_1 =', cost)

def gradient_descent(X, y, theta, alpha, iterations):
    """
    Compute the optimal parameters using gradient descent for linear regression.

    Parameters:
    X : 2D array where each row represents the training example and each column represents the feature
        m = number of training examples
        n = number of features (including X_0 column of ones)
    y : 1D array of labels/target values for each training example. dimension(m)
    theta : 1D array of fitting parameters or weights. Dimension (n)
    alpha : Learning rate (scalar)
    iterations : Number of iterations (scalar)

    Returns:
    theta : Updated values of fitting parameters or weights after 'iterations' iterations. Dimension (n)
    cost_history : Array containing the cost for each iteration. Dimension (iterations)
    """

    m = len(y)  # Number of training examples
    cost_history = np.zeros(iterations)

    for i in range(iterations):
        predictions = X.dot(theta)
        errors = np.subtract(predictions, y)
        sum_delta = (alpha / m) * X.transpose().dot(errors)
        theta -= sum_delta
        cost_history[i] = compute_cost(X, y, theta)

    return theta, cost_history

theta = [0., 0., 0., 0]
iterations = 1500
alpha = 0.01

theta, cost_history = gradient_descent(X, y, theta, alpha, iterations)
print('Final value of theta =', theta)
print('cost_history =', cost_history)


# Assuming that X, y, and theta are already defined
# Also assuming that X has two columns: a feature column and a column of ones

# Scatter plot for the training data
plt.scatter(X[:, 0], y, color='red', marker='+', label='Training Data X1')
plt.scatter(X[:, 1], y, color='blue', marker='x', label='Training Data X2')
plt.scatter(X[:, 2], y, color='purple', marker='*', label='Training Data X3')

# Line plot for the linear regression model
plt.plot(X[:, 1], X.dot(theta), color='green', label='Linear Regression')
#plt.plot(X[:, 0:2], X.dot(theta), color='green', label='Linear Regression')
#plt.plot(X[:, 2], X.dot(theta), color='green', label='Linear Regression')

# Plot customizations
plt.rcParams["figure.figsize"] = (10, 6)
plt.grid(True)
plt.xlabel('X')
plt.ylabel('Y')
plt.title('Linear Regression Fit 0.01 Alpha')
plt.legend()

# Show the plot
plt.show()




plt.plot(range(1, iterations + 1), cost_history, color='blue')
plt.rcParams["figure.figsize"] = (10, 6)
plt.grid(True)

plt.xlabel('Number of iterations')
plt.ylabel('Cost (J)')
plt.title('Convergence of gradient descent 0.01 Alpha')

# Show the plot
plt.show()
